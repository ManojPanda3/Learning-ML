{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Locally Weighted Regression\n",
        "\n",
        "lecture:\n",
        "\n",
        "[![Video Title](https://img.youtube.com/vi/het9HFqo1TQ/mqdefault.jpg)](https://www.youtube.com/watch?v=het9HFqo1TQ)\n",
        "\n",
        "### 1. Paramatric Learning Algorithm\n",
        "> Fit a fixed ammount of parameters to data\n",
        "\n",
        "### 2. NonParamatric Learning Algorithm\n",
        "> Ammount of parameters grows(mostly linear manner) with the size of training sets.\n",
        "\n",
        "\n",
        "in Locally weighted regression Fit $\\theta$ to minimize A modified cost function\n",
        "$$ \\sum_{i=1}^{m}{w^i (y^i - θ^T x^i)} $$\n",
        "where $w^i$ is a weighted function\n",
        "$$ w^i = e^{-\\frac{(x^i - x)^2}{2τ^2}} $$\n",
        "if $|x^i - x| ≈ 0 → w^i ≈ 1$\n",
        "\n",
        "if $|x^i - x| ≈ ∞ → w^i ≈ 0$"
      ],
      "metadata": {
        "id": "Ep6cgtBbPpwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilistic Interpulation\n",
        "**Why least squares ?**\n",
        "\n",
        "housing price is said to be in formate of:\n",
        "$$y^i = θ^Tx^i + E^i$$\n",
        "here $E^i$ is the error term which adds the noise in the market.\n",
        "\n",
        "(here noise is: Like someday multiple buyer want to buy the targeted house so the values goes up. But thats temporary since the buyer can move on to another house nearby )\n",
        "$$E^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n",
        "$$ P(E^i) = \\frac{e^{-\\frac{(E^i)^2}{2σ^2}}}{\\surd{(2π)} \\sigma} $$\n",
        "\n",
        "we assuming that these $E^i$ term is IID (Indipendently And Identically Distributed)\n",
        "\n",
        "This implies:\n",
        "$$ P(y^i | x^i; Θ) = \\frac{e^{-\\frac{(y^i - θ^Tx^i)^2}{2σ^2}}}{\\surd{(2π)} \\sigma} $$\n",
        "here ; means `parameterized as`\n",
        "\n",
        "$$L(\\theta) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "here $L(\\theta)$ is likelihood of parameter $\\theta$\n",
        "\n",
        "### Log of likelihood\n",
        "$$ l(\\theta) = log(L(\\theta)) $$\n",
        "$$ m log(\\frac{1}{√2\\pi σ}) + \\sum_{i=1}^{m} \\frac{-(y^i - \\theta^T x^i)^2}{2σ^2 } $$\n",
        "\n",
        "So if we need to increase the likelihood of $\\theta$ for our X and y we need to maximize the likelihood of $l(\\theta)$ which means $\\theta$ which"
      ],
      "metadata": {
        "id": "dY-wFRTGrwOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Problem\n",
        "\n",
        "## Binary Classification\n",
        "\n",
        "here the possible outcome is 2\n",
        "example: Titanic\n",
        "\n",
        "### Logistic Regression\n",
        "we want the output to be\n",
        "$$ h_{θ}(x) ∈ [0,1] $$\n",
        "$$ h_{θ}(x) = g(θ^Tx) = \\frac{1}{1 + e^{-θ^Tx}}$$\n",
        "\n",
        "so lets assume $θ^Tx = z$\n",
        "then\n",
        "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
        "\n",
        "here g(z) is **\"logistic function\"** or **\"sigmoid function\"**\n",
        "\n",
        "$$ P(y=1|x;Θ) = h_Θ(x)$$\n",
        "$$ P(y=0|x;Θ) = 1 - h_Θ(x)$$\n",
        "\n",
        "$$ P(y|x;Θ) = h_θ^{y}(x) (1-h_θ(x))^{1-y}$$\n",
        "\n",
        "LOG likelihood of this:\n",
        "$$ l(Θ) = P(y|x;θ) = Π_{i=1}^m P(y^i|x^i|θ)$$\n",
        "$$ log(l(Θ)) = \\sum_{i=1}^{m}{y^i log(h_Θ(x^i))}+ (1-y^i)log(1 - h_θ(x^i)) $$\n",
        "\n",
        "\n",
        "we need to choose the best $Θ$ to maximize the $l(θ)$\n",
        "$$ \\theta_j := \\theta_j + \\frac{\\partial l(θ)}{\\partial \\theta_j}$$\n",
        "$$ \\theta_j := \\theta_j + α \\sum_{i=1}^m(y^i - h_θ(x^i))x_j^i $$\n"
      ],
      "metadata": {
        "id": "NnQYHAxf_Mzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method\n",
        "\n",
        "we have f and we want to find $\\theta$ so that f(Θ) = 0 [ want to minimize l(Θ) ie we want to $ l^{'}(\\theta) = 0 $\n",
        "\n",
        "$$ \\theta^1 := \\theta^0 - Δ $$\n",
        "$$ f'(θ^0) = f(\\theta^0) / Δ $$\n",
        "$$ Δ = f(θ^0) / f'(θ^0) $$\n",
        "$$ \\theta^{t+1} := \\theta^t -  f(θ^t) / f'(θ^t) $$\n",
        "\n",
        "Lets do\n",
        "$$ f(\\theta) = l'(\\theta) $$\n",
        "$$ \\theta^{t+1} := \\theta^t -  l'(θ^t) / l''(θ^t) $$"
      ],
      "metadata": {
        "id": "kRfxKmjYQYLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quadratic Convergence\n",
        "\n",
        "In newtons method as we goes near the minimum the error converges quadratically thats called Quadratic Convergence\n",
        "\n",
        "$$ \\theta^{t+1} := \\theta^{t} + H^{-1}\\nabla_{\\theta}l $$\n",
        "where H is hessian matrix\n",
        "\n",
        "$$ H_{ij} = \\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_j} $$"
      ],
      "metadata": {
        "id": "HjZ30wfTC-WI"
      }
    }
  ]
}